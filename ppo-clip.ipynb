{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import eplus_env\n",
    "import tqdm as tqdm\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "# from network import MLP\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "\n",
    "# import faulthandler\n",
    "# faulthandler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.py\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This MLP will be used as both the actor and the critic.\n",
    "    in_dim: shape of the observation\n",
    "    out_dim: number of actions that can be taken\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 30)\n",
    "        self.fc2 = nn.Linear(30, 30)\n",
    "        self.fc3 = nn.Linear(30, out_dim)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        x = F.relu(self.fc1(observation))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('doe_large_office_chennai_data_gen-v0')\n",
    "info, state, done = env.reset()\n",
    "\n",
    "in_dim = len(state)\n",
    "# Flattened action space\n",
    "out_dim = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo.py\n",
    "class EPPPO:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.obs_dim = in_dim\n",
    "        self.act_dim = out_dim\n",
    "        \n",
    "        # Initializing Actor and Critic networks\n",
    "        self.actor = MLP(self.obs_dim, self.act_dim)\n",
    "        self.critic = MLP(self.obs_dim, 1)\n",
    "\n",
    "        self.initiate_hyperparameters()\n",
    "\n",
    "        self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
    "        self.cov_mat = torch.diag(self.cov_var)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get the mean action from the actor network. \n",
    "        Create a multivariate normal distribution \n",
    "        Sample an action from the distribution along with it's logprob\n",
    "        Return sampled action and log prob\n",
    "        \"\"\"\n",
    "        mean = self.actor(state)\n",
    "         \n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)    \n",
    "\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "\n",
    "    def learn(self, max_timesteps):\n",
    "        \"\"\"\n",
    "        Imitiating Stable Baselines approach, with max_timesteps instead of epochs\n",
    "        \"\"\"\n",
    "        timestep_count = 0\n",
    "        while timestep_count < max_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "\n",
    "    def init_hyperparams(self):\n",
    "        self.max_timesteps_per_batch = 1056*10\n",
    "        self.max_timesteps_per_episode = 1056\n",
    "        self.gamma = 0.95\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        Environment specific action sampler for energyplus. \n",
    "        \"\"\"\n",
    "        possible_actions = [5, 6, 7, 8, 9, 10]\n",
    "        possible_actions2 = [5, 6, 7, 8, 9, 10]\n",
    "\n",
    "        all_possible_actions = [[i, j] for i in possible_actions for j in possible_actions2]\n",
    "        return random.choice(all_possible_actions)\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        Simple reward calculator for the energyplus environment. \n",
    "        \"\"\"\n",
    "        reward = (state[-16]* 2.77778e-7)+ (state[-15]* 2.77778e-7)+ (state[-17]* 2.77778e-7)+ (state[-14]* 2.77778e-7)+ (state[-13]* 2.77778e-7)+ (state[-12]* 2.77778e-7)+ (state[-11]* 2.77778e-7)+ (state[-10]* 2.77778e-7)+ (state[-9]* 2.77778e-7)\n",
    "        # reward = (reward * -1)\n",
    "        return reward\n",
    "    \n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        \"\"\"\n",
    "        Returns the rewards to go per episode per batch\n",
    "        \"\"\"\n",
    "        batch_rtgs = []\n",
    "\n",
    "        for ep_rewards in reversed(batch_rews):\n",
    "            discounted_reward = 0\n",
    "            for reward in reversed(ep_rewards):\n",
    "                discounted_reward = reward + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        \n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float32)\n",
    "        return batch_rtgs\n",
    "\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        One iteration of policy improvement. \n",
    "        \"\"\"\n",
    "        batch_obs = []\n",
    "        batch_actions = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "        \n",
    "        batch_timesteps = 0\n",
    "        while batch_timesteps < self.max_timesteps_per_batch:\n",
    "            ep_rewards = []\n",
    "            env = gym.make('doe_large_office_chennai_data_gen-v0')\n",
    "            info, state, done = env.reset()\n",
    "            for timestep in range(self.max_timesteps_per_episode):\n",
    "                batch_timesteps += 1\n",
    "                batch_obs.append(state)\n",
    "                action, log_prob = self.get_action(state)\n",
    "                info, state, done = self.env.step(action)\n",
    "\n",
    "                ep_rewards.append(get_reward(state))\n",
    "                batch_actions.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    break\n",
    "            batch_lens.append(timestep+1)\n",
    "            batch_rews.append(ep_rewards)\n",
    "\n",
    "        batch_obs = torch.tensor(batch_obs, dtype=torch.float32)\n",
    "        batch_actions = torch.tensor(batch_actions, dtype=torch.float32)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32)\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "\n",
    "        return batch_obs, batch_actions, batch_log_probs, batch_rtgs, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinergym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
